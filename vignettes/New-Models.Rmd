---
title: "New-Models"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{New-Models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(GOF)
```

Usually, one creates a GOF-model-test-class via the function
*GOF_model()*. But this function is actually a wrapper for the
class *GOF_model_test*. That class needs other classes to
work properly. In particular it uses the three interfaces
*GOF_model_info_extractor*, *GOF_model_simulator*,
*GOF_model_trainer*. In general, objects of class *lm* or
*glm* can be used with *GOF_model*. However, there might 
be situations where one has to overwrite the default
behavior. For instance, if you want to apply the methodology
to new models or simply because a model fit returned by an
R-package does not work properly with the GOF-package. 
Here we show how to implement the three interfaces for a
concret case, i.e. a model fitted by MASS::glm.nb.

First we generate a data set
and fit a negative binomial model in order to apply the implemented
interfaces to that fit. Although the user does not have to call those
functions manually in order to use the Goodness-of-Fit test, we hope 
that this make the process less abstract.

```{r}
library(MASS)
set.seed(1)
X1 <- rnorm(100)
X2 <- rnorm(100)
d <- data.frame(
  y = MASS::rnegbin(n = 100, mu = exp(0.2 + X1 * 0.2 + X2 * 0.6), theta = 2),
  x1 = X1,
  x2 = X2)
fit <- MASS::glm.nb(y~x1+x2, data = d)
```

The first interface requires
that we implement three functions *yhat*, *y_minus_yhat* and
*beta_x_covariates*, which are the predictions for the dependent
variable (also called target-variable), the residuals on the scale
of the dependent variable and the inner product of the estimated
parameters and the independent variables (also called covariates or
features).

```{r}
my_negbin_info_extractor <- R6::R6Class(
  classname = "my_negbin_info_extractor",
  inherit = GOF_model_info_extractor,
  public = list(
    yhat = function(model) {
      predict.glm(object = model, type = "response")
    },
    y_minus_yhat = function(model) {
      residuals.glm(object = model, type = "response")
    },
    beta_x_covariates = function(model) {
      predict.glm(object = model, type = "link")
    }
  ))
my_info_extractor <- my_negbin_info_extractor$new()
```

Lets look at the first few data points:

```{r}
head(d)
```

Now we are able to predict $y$:

```{r}
head(my_info_extractor$yhat(model = fit))
```

And calculate the difference between $y$ and our prediction:

```{r}
head(my_info_extractor$y_minus_yhat(model = fit))
```

And based on the estimated coefficients we can calculate their 
inner product with $x_1$ and $x_2$ (Note that the model has
an intercept so for the inner product one has to think of
an extra column in the data set that contains only ones!)

```{r}
head(my_info_extractor$beta_x_covariates(model = fit))
```

The next interface requires that we implement the function
*resample_y* which is a function that generates new
dependent variable according to a fitted negative binomial
model

```{r}
my_negbin_simulator <- R6::R6Class(
  classname = "my_negbin_simulator",
  inherit = GOF_model_simulator,
  public = list(
    resample_y = function(model) {
      simulate(model)[,1]
    }
  )
)
my_simulator <- my_negbin_simulator$new()
```

This implemented interface generates as many observations
as are contained in the data set used to fit the model
following the fitted model.

Again looking at first at the original data points:

```{r}
head(d)
```
Now lets look at new $y$s generated according to the fitted
model, i.e. following a negative binomial distribution
subject to the independent variables:

```{r}
head(my_simulator$resample_y(model = fit))
```

Finally, we need to implement the interface
*GOF_model_trainer* which requires a function
*refit* that is able to update the model 
object by refitting it to a new data set.

```{r}
my_negbin_trainer <- R6::R6Class(
  classname = "GOF_glmnb_trainer",
  inherit = GOF_model_trainer,
  public = list(
    refit = function(model, data) {
      MASS::glm.nb(formula = formula(model), data = data)
    }))
my_trainer <- my_negbin_trainer$new()
```

Of course, fitting the model again to the original data 
results in the same fitted model. With the defined classes
we can now easily generate a new data set and refit the
model to that new data set.

```{r}
new_data <- d
new_data$y <- my_simulator$resample_y(model = fit)
my_trainer$refit(model = fit, data = new_data)
```

Now all ingredients are available for applying the 
Goodness-of-Fit test:

```{r}
set.seed(1)
mt <- GOF_model_test$new(
  model = fit,
  data = d,
  nmb_boot_samples = 100,
  y_name = "y",
  Rn1_statistic = Rn1_CvM$new(),
  gof_model_info_extractor = my_info_extractor,
  gof_model_resample = GOF_model_resample$new(
    gof_model_simulator = my_simulator,
    gof_model_trainer = my_trainer
  )
)
mt$get_pvalue()
```

Since the specified model family is correct we would expect a
non-small pvalue. For sake of completness, lets compare it to
the p-value generated by *GOF_model()*:

```{r}
set.seed(1)
mt2 <- GOF_model(
  model = fit,
  data = d,
  nmb_boot_samples = 100,
  simulator_type = "parametric",
  y_name = "y",
  Rn1_statistic = Rn1_CvM$new()
)
mt2$get_pvalue()
```

# Some notes on the negative binomial model

A negative binomial model is a generalized linear model. 
Furthermore, within R MASS::glm.nb returns an object of class 
*glm* and this package actually can process *glm*-classes. 
However, MASS::glm.nb seems to have a bug that prevents 
to propoerly update/refit such an object via the stats::update()
function. For instance, the model we fitted at the beginning of
this vignette did not used the *init.theta* parameter but the 
function-call stored in the fit object explicitly uses this 
parameter:

```{r}
fit
```

Although in the above example both p-values for the GOF-test
were the same, it is probably safer to provide an implementation
of the interface *GOF_model_trainer* for MASS::glm.nb that reflects
as close a possible the way the original model was fitted.
The other two interfaces do not need a reimplementation because
the package already provides them (we implemented them only
for illustrative purpose)!

```{r}
mt <- GOF_model_test$new(
  model = fit,
  data = d,
  nmb_boot_samples = 100,
  y_name = "y",
  Rn1_statistic = Rn1_CvM$new(),
  gof_model_info_extractor = GOF_glm_info_extractor$new(),
  gof_model_resample = GOF_model_resample$new(
    gof_model_simulator = GOF_glm_sim_param$new(),
    gof_model_trainer = my_trainer
  )
)
mt$get_pvalue()
```
